\documentclass[a4paper]{article}
\usepackage{textpos}
\usepackage{url}
\usepackage{graphicx}
\usepackage{array}
\usepackage{cite}
\usepackage{tikz}
\usetikzlibrary{shapes}
\usepackage{courier}
\usepackage[margin=1.4in, paperwidth=8.5in, paperheight=11in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{chngcntr}
\counterwithin{figure}{section}
\counterwithin{table}{section}
\counterwithin{equation}{section}
\setlist[enumerate]{itemsep=0mm}
\setlist[itemize]{itemsep=0mm}
\newcommand{\fillin}[1]{X}

\begin{document}

\title{Neural Networks for Small-Scale Combat Optimization in Real-Time Strategy Games}
\author{Jacob Menashe and Vineet Keshari}
\maketitle

\begin{abstract}
A major challenge for players and AI modules alike in the real-time strategy community is the effective control of individual units in combat scenarios. Starcraft, a real-time strategy game released by Blizzard Entertainment, provides built-in heuristics for assisting players in combat management, but these heuristics are limited in their ability to make full use of a unit's potential. We present a series of techniques for training neural networks in small-scale combat situations, and lay the groundwork for further improvements to this system. We present a neural network design for controlling combat units, which, when coupled with the NEAT evolutionary learning algorithm, is able to consistently overcome Starcraft's built-in AI. In most cases our technique achieves optimal champion performance in less than 100 generations, and scales to large battles in a variety of unit configurations. Our system achieves a \fillin{pct games against AI, final}\% win rate against the AI, and a \fillin{pct games against humans}\% win rate against humans in small-scale battles.
\end{abstract}

\section{Introduction}

Micro-management is a major component of real-time strategy (RTS) games such as Starcraft. Players must effectively move individual units around for gathering resources, constructing buildings, or for tactical combat. In the case of tactical combat, micro-management requires quick action on the part of the player and thus can be difficult to master even with a small number of units. While modern games tend to minimize the degree to which players must control units on a small scale through the implementation of batch commands, this is usually achieved with simple heuristics a significant amount of potential is sacrificed for such enhancements. For a simple example of this, consider the following scenario: a player might take a heterogeneous group of close-range and long-range units and issue a batch attack command, at which point her units will either attack at random or attack one unit at a time. A more effective strategy might be to allow ranged units to disengage and re-engage based on the enemy's response, to allow these units to keep their distance, while close-range units focus on the greatest threat to allied ranged units. In this situation, the simplicity of the batch AI has diminished the effectiveness of the player's army. Such circumstances are prevalent in modern RTS games.

The domain of micro-management in combat environments is therefore well-suited to the application of intelligent, autonomous agents. While the best AI implementations still fall below the average competitive human player in terms of high-level planning and strategy \cite{weber2011building}, a trained unit controller agent should have little difficulty outperforming a human for any reasonably-sized army. Indeed, while an agent may potentially make hundreds of decisions and unit actions per second, even the most skilled human Starcraft players can only achieve about 5 actions per second \cite{mccoy2008integrated}.

In this paper we present a novel application of NEAT to small-scale combat problem, restricted to the domain of Starcraft. We devise a variety of network topologies and configurations for outperforming the game's built-in AI, as well as human players, and describe how our technique may be extended for outperforming winning bots from the Starcraft AI Competition \fillin{ai competition citation}. In Section \ref{sec:back} we discuss some background to AI development in real-time strategy games and specifically in Starcraft, and examine another body of work that focuses on small-scale combat. In Section \ref{sec:soft} we review the software libraries used for our analysis, and continue with a discussion of our approach to learning controller agents in Section \ref{sec:tech}. We evaluate our results agains the built-in Starcraft AI, against other bots from the Starcraft AI Competition, and other humans in Section \ref{sec:exp}. In Sections \ref{sec:conclusion} and \ref{sec:future} we conclude and discuss additional updates and improvements to our method, toward the ultimate goal of providing a general-use combat manager.

\section{Background}
\label{sec:back}

Classic AI techniques (or GOFAI) have been used on a variety of board games, such as Connect Four, Checkers, and Chess \cite{Allis94searchingfor}. Due to their simplistic and discrete state spaces, board games can often be broken down by traditional AI techniques such as alpha-beta pruning, or in some cases even brute force search. Chess pushes the limits of these techniques with a search tree complexity of at least $10^{120}$ \cite{Shannon:1988:PCP:61701.67002}, but can still benefit from the prediction value of limited search. What these games have in common is that computers maintain a significant advantage over humans stemming from their predictive abilities. As computing technology has improved, however, games have begun to take on more realistic simulations of the world and therefore rely heavily on continuous and highly complex states and actions. This increase in state-action complexity has in turn rendered traditional AI approaches ineffective at performing competitively with human opponents.

Recent work has seen reasonable success at taking on the challenges of modern games and simulations, particularly in the domain of First-Person Shooters (FPS). FPS games are a genre of computer game in which the player views the world through an avatar from a first-person point of view. The point of an FPS is to navigate through the game world and kill opponents with weapons and tools found throughout the environment. Neuroevolution has been successfully applied toward the goal of producing human-like actions \cite{schrum:cig11competition}, resulting in creating bots that effectively pass the Turing test. In FPS games, creating bots that are simply better than human players can be achieved through simple physics calculations, so here the emphasis is on enabling human-like movements and downgrading combat prowess where it can be seen as unrealistic. In contrast, our work focuses on outperforming human- and AI-controlled systems alike.

There has been considerable progress toward this goal in Starcraft, primarily due to the Starcraft AI Competition. The competition pits Starcraft bots against one another to determine a winner, however this task lies parallel to that of defeating a human controller. EISBot, one of the more successful bot implementatoins, has been tested against skilled human players and achieved a win rate of approximately 32\% \cite{weber2011building}. Other top-performing bots from the competition have been unable to win against expert players \cite{ai_comp_2010_website}. In our work, we therefore train and evaluate our networks based on the built-in AI, with the long-term goal of competing against both bots in the competition and human players.

Typical approaches to Starcraft bots include case-based reasoning \fillin{more citations} and hierarchical skill management \fillin{citations}. The former technique aims to learn skills, build order, and general strategy from replays of past games, while the latter technique organizes expert modules into a hierarchical decision model. These two approaches need not be mutually exclusive. Wender and Watson refine the problem of creating expert modules by focusing exclusively on small-scale combat tactics in \cite{rl_small_scale_combat}, and it is this work that relates most closely to our own. While Wender and Watson restrict their analysis to the particular combat configuration of a single, fast, ranged unit versus multiple slower short-ranged units, we seek to optimize small-scale combat in a more general sense by allowing for heterogeneous unit groups with an unspecified number of combatants. In this way, our approach moves toward a more flexible solution for the problem of micro-management in small-scale combat. 

Another notable difference between our work and that of \cite{rl_small_scale_combat} is our use of NEAT (see Section \ref{sec:neat}) for learning unit controllers. We select NEAT for a number of reasons. First, as we will describe in Section \ref{sec:net_design}, our state space is continuous and high-dimensional. Neural networks are naturally well-designed for parsing continuous state spaces and determining their value. Second, due to the computational cost of communicating with and executing commands on the Starcraft runtime, we are restricted in the number of iterations we may perform to learn this state space. NEAT has been shown to significantly outperform traditional reinforcement learning methods under these circumstances \cite{Stanley:2004:EEN:1048234}. Finally, while we can accurately determine the value of a match, we cannot predict what intermediate behaviors might be optimal for our overall goal of achieving combat prowess. We therefore rely on NEAT to discover these innovations in the form of connections and connection weights in an efficient manner.

\section{Software}
\label{sec:soft}

Our research was enabled by the BroodWar API (BWAPI) \cite{bwapi}, BWAPI Mono-Bridge \cite{monobridge}, and Starcraft: Broodwar itself. We use SharpNeat \cite{sharpneat}, a C\# NEAT implementation for neuroevolution. While BWAPI is natively coded in C++, we selected a C\# implementation due to the following considerations:

\begin{itemize}
	\item Our focus for this work is in testing new configurations for NEAT and new combinations with our own heuristics, so development speed is of particular importance. C\# provides many high-level language features that allow for faster development than in C++.
	\item BWAPI only runs on Visual Studio in Windows, so there is no cross-platform or open-source advantage to using C++.
	\item SharpNEAT is an exceptionally well-coded and well-documented NEAT implementation. Along with working quite simply out of the box, the code is modularized and easily configurable, and has built in analysis and visualization tools.
\end{itemize}

Because C\# communicates with BWAPI through a client-server interface, processing is slower than would be in the case of C++. We felt that the increased productivity more than made up for this disadvantage. In addition, the mono-bridge implementation uses an out-of-date version of BWAPI, although this didn't significantly affect our work. For future development it will be necessary to update this interface.

\section{Technical Approach}
\label{sec:tech}

We now cover our approach to the small-scale combat problem, beginning with a short introduction to NeuroEvolution of Augmenting Topologies (NEAT). We continue with a discussion of our implementation and network designs before proceeding to our software architecture.

\subsection{The NEAT Algorithm}
\label{sec:neat}

NEAT provides us with a fairly simple way of creating network designs without prior knowledge of the optimal target configuration. While traditional neural network algorithms, such as backpropagation, rely on a predefined network architecture, NEAT requires only that we specify a relatively small number of parameters, shown in Table \ref{tab:neat_params}. Hidden nodes are added as needed based on random mutation and resulting fitness values.

\begin{table}
\centering
\begin{tabular}{|l|l|}
	\hline
	{\bf Parameter} & {\bf Interpretation}\\ \hline
	Input Nodes & Game State Representation\\ \hline
	Output Nodes & Available Actions and Decisions\\ \hline
	\multirow{2}{*}{Evolution Parameters} &  Intensity and Character of\\
	& Behavior Adjustments\\ \hline
\end{tabular}
\caption{Parameters specified for the NEAT algorithm.}
\label{tab:neat_params}
\end{table}

Clearly, properly designing the input and output nodes is critical to the performance of a neural network. We therefore maintain the evolutionary parameters defined in Table \ref{tab:neat_params_specific} for all tests, and instead focus our attention on the network I/O architecture.

\begin{table}
\centering
\begin{tabular}{|l|l|}
	\hline
	Connection Weight Mutation Probability & 0.8\\ \hline
	Add Connection Probability & 0.1\\ \hline
	Delete Connection Probability & 0.001\\ \hline
	Add Node Probability & 0.1\\ \hline
	Timesteps Per Activation & 10\\ \hline
	Connection Cycles & Enabled (i.e. Recurrent)\\ \hline
\end{tabular}
\caption{Selected NEAT evolution parameters. Probabilities are normalized such that they sum to 1.0 upon evaluation.}
\label{tab:neat_params_specific}
\end{table}

\subsection{Training Setup}

Our network designs are closely tied to our training configurations, and thus we will cover the general outline for training. Starcraft's built-in Campaign Editor application allows for users to create custom maps complete with environment specifications and state-based triggers. We used the campaign editor to construct scenarios in which teams of units are repeatedly spawned at two starting points, one for our bot (``allies") and one for the built-in AI (``enemies"). Allies and enemies repeatedly fight one another until all of one side has been defeated, signaling the completion of that round. For each genome generated by NEAT, we assign fitness based on its average performance over 5 rounds using Equation \ref{eqn:fitness}:

\begin{equation}
f = \frac{(A - E) + A_\mathrm{max}}{A_\mathrm{max}}\\
\label{eqn:fitness}
\end{equation}

where $A$ and $E$ are the ally and enemy counts at the end of the round, respectively, and $A_\mathrm{max} = E_\mathrm{max}$ is the starting count of allies or enemies in the training scenario. Using this formulation, the fitness value is always in the range $[0,2]$, with 1 representing a tie. In practice, ties only occur when the learned network moves the allies out of the combat area, so to avoid this behavior we assign a fitness score of 0 in all scenarios where $A \neq 0$ and $E \neq 0$.

\subsection{Network Design}
\label{sec:net_design}

We attempted multiple network designs, each of which is evaluated in Section \ref{sec:exp}. In this section we make multiple references to training time in terms of hours. Training was performed on a desktop PC with Starcraft and NEAT running on a single core at 3.07 GHz. 

\subsubsection{Multi-Unit Control}

We now begin with the multi-unit control network design in Figure \ref{fig:design_multiunit}.

\begin{figure}
\input{multiunit_orig}
\caption{The multi-unit network design for 3 allies and 3 enemies. $r$ and $\theta$ represent relative distance and angle. $s$ represents a score in the range $[0,1]$, and $f$ is a flip.}
\label{fig:design_multiunit}
\end{figure}

This design represents our initial attempt at a network design. Inputs to the network include health percentages for each ally and each enemy in the training session, as well as the distances and angles of each enemy relative to each ally. Outputs include one score for each ally/enemy pair, as well as move score, distance, angle, and flip values for each ally. Flips are added as a way to differentiate between different extremes of the output range of the movement angle node. Originally we attempted a direct mapping of $\theta = 2 \cdot \pi \cdot o_\theta$, where $o_\theta \in [0,1]$ is the value of the movement angle output node. This has the effect that the extremes 0 and 1 map to the same movement. Thus, we instead use the following formulation:

 \begin{displaymath}
   \theta = \left\{
     \begin{array}{lr}
       o_\theta \cdot \pi & : f > 0.5\\
       o_\theta \cdot -\pi & : f \leq 0.5\\
     \end{array}
   \right.
\end{displaymath}

This way, units can move omnidirectionally with either extreme of the output range corresponding to different directions.

Using this network design, our units were able to achieve reasonable fitness levels in almost all trials with small combat scenarios within a few hours of training. Particularly, the design performed well with 3-5 units on either side. In the case of a 20-vs-20 battle, however, the design was not able to surpass an average population fitness of 1, meaning our units were losing battles more often than not, even after days of training. Reflecting on the design, this makes sense: with 20 allies and enemies, there are $2 \cdot 20 + 20\cdot 20 \cdot 2 = 880$ input nodes and $20 \cdot (20 + 4) = 480$ output nodes. This results in over 400,000 possible connections, without taking into account hidden nodes that are added through neuroevolution. Given the constraints of interoperation with Starcraft, this presents a clear problem for quickly evolving networks to solve the small-scale combat problem. Moreover, the evolved networks are statically tied to the number of combatants; for any particular battle, we would need to have a precise controller pre-trained for that particular scenario. In practice, we would like our controllers to be more flexible, and thus we proceed to other solutions.

\subsubsection{Individual Control}

\begin{figure}
\input{individualcontroller}
\caption{The individual controller network design.}
\label{fig:design_individual}
\end{figure}

\subsubsection{Squad Control}

\begin{figure}
\input{squadcontroller}
\caption{The squad controller network design.}
\label{fig:design_squad}
\end{figure}

\begin{figure}
\input{squadmanager}
\caption{The squad manager network design.}
\label{fig:design_squad_manager}
\end{figure}

\section{Experiments}
\label{sec:exp}

\section{Conclusions}
\label{sec:conclusion}

\section{Future Work}
\label{sec:future}

\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}
