\documentclass[a4paper]{article}
\usepackage{url}
\usepackage{graphicx}
\usepackage{array}
\usepackage{cite}
\usepackage{tikz}
\usepackage{courier}
\usepackage[margin=1.4in, paperwidth=8.5in, paperheight=11in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{chngcntr}
\counterwithin{figure}{section}
\counterwithin{table}{section}
\setlist[enumerate]{itemsep=0mm}
\setlist[itemize]{itemsep=0mm}
\newcommand{\fillin}[1]{X}

\begin{document}

\title{Neural Networks for Small-Scale Combat Optimization in Real-Time Strategy Games}
\author{Jacob Menashe and Vineet Keshari}
\maketitle

\begin{abstract}
A major challenge for players and AI modules alike in the real-time strategy community is the effective control of individual units in combat scenarios. Starcraft, a real-time strategy game released by Blizzard Entertainment, provides built-in heuristics for assisting players in combat management, but these heuristics are limited in their ability to make full use of a unit's potential. We present a series of techniques for training neural networks in small-scale combat situations, and lay the groundwork for further improvements to this system. We present a neural network design for controlling combat units, which, when coupled with the NEAT evolutionary learning algorithm, is able to consistently overcome Starcraft's built-in AI. In most cases our technique achieves optimal champion performance in less than 100 generations, and scales to large battles in a variety of unit configurations. Our system achieves a \fillin{pct games against AI, final}\% win rate against the AI, and a \fillin{pct games against humans}\% win rate against humans in small-scale battles.
\end{abstract}

\section{Introduction}

Micro-management is a major component of real-time strategy (RTS) games such as Starcraft. Players must effectively move individual units around for gathering resources, constructing buildings, or for tactical combat. In the case of tactical combat, micro-management requires quick action on the part of the player and thus can be difficult to master even with a small number of units. While modern games tend to minimize the degree to which players must control units on a small scale through the implementation of batch commands, this is usually achieved with simple heuristics a significant amount of potential is sacrificed for such enhancements. For a simple example of this, consider the following scenario: a player might take a heterogeneous group of close-range and long-range units and issue a batch attack command, at which point her units will either attack at random or attack one unit at a time. A more effective strategy might be to allow ranged units to disengage and re-engage based on the enemy's response, to allow these units to keep their distance, while close-range units focus on the greatest threat to allied ranged units. In this situation, the simplicity of the batch AI has diminished the effectiveness of the player's army. Such circumstances are prevalent in modern RTS games.

The domain of micro-management in combat environments is therefore well-suited to the application of intelligent, autonomous agents. While the best AI implementations still fall below the average competitive human player in terms of high-level planning and strategy \cite{weber2011building}, a trained unit controller agent should have little difficulty outperforming a human for any reasonably-sized army. Indeed, while an agent may potentially make hundreds of decisions and unit actions per second, even the most skilled human Starcraft players can only achieve about 5 actions per second \cite{mccoy2008integrated}.

In this paper we present a novel application of NEAT to small-scale combat problem, restricted to the domain of Starcraft. We devise a variety of network topologies and configurations for outperforming the game's built-in AI, as well as human players, and describe how our technique may be extended for outperforming winning bots from the Starcraft AI Competition \fillin{ai competition citation}. In Section \ref{sec:back} we discuss some background to AI development in real-time strategy games and specifically in Starcraft, and examine another body of work that focuses on small-scale combat. In Section \ref{sec:tech} we review our approach to learning controller agents, along with a discussion of software libraries and implementation challenges in Section \ref{sec:soft}. We evaluate our results agains the built-in Starcraft AI, against other bots from the Starcraft AI Competition, and other humans in Section \ref{sec:exp}. In Sections \ref{sec:conclusion} and \ref{sec:future} we conclude and discuss additional updates and improvements to our method, toward the ultimate goal of providing a general-use combat manager.

\section{Background}
\label{sec:back}

Classic AI techniques (or GOFAI) have been used on a variety of board games, such as Connect Four, Checkers, and Chess \cite{Allis94searchingfor}. Due to their simplistic and discrete state spaces, board games can often be broken down by traditional AI techniques such as alpha-beta pruning, or in some cases even brute force search. Chess pushes the limits of these techniques with a search tree complexity of at least $10^{120}$ \cite{Shannon:1988:PCP:61701.67002}, but can still benefit from the prediction value of limited search. What these games have in common is that computers maintain a significant advantage over humans stemming from their predictive abilities. As computing technology has improved, however, games have begun to take on more realistic simulations of the world and therefore rely heavily on continuous and highly complex states and actions. This increase in state-action complexity has in turn rendered traditional AI approaches ineffective at performing competitively with human opponents.

Recent work has seen reasonable success at taking on the challenges of modern games and simulations, particularly in the domain of First-Person Shooters (FPS). FPS games are a genre of computer game in which the player views the world through an avatar from a first-person point of view. The point of an FPS is to navigate through the game world and kill opponents with weapons and tools found throughout the environment. Neuroevolution has been successfully applied toward the goal of producing human-like actions \cite{schrum:cig11competition}, resulting in creating bots that effectively pass the Turing test. In FPS games, creating bots that are simply better than human players can be achieved through simple physics calculations, so here the emphasis is on enabling human-like movements and downgrading combat prowess where it can be seen as unrealistic. In contrast, our work focuses on outperforming human- and AI-controlled systems alike.

There has been considerable progress toward this goal in Starcraft, primarily due to the Starcraft AI Competition. The competition pits Starcraft bots against one another to determine a winner, however this task lies parallel to that of defeating a human controller. EISBot, one of the more successful bot implementatoins, has been tested against skilled human players and achieved a win rate of approximately 32\% \cite{weber2011building}. Other top-performing bots from the competition have been unable to win against expert players \cite{ai_comp_2010_website}. In our work, we therefore train and evaluate our networks based on the built-in AI, with the long-term goal of competing against both bots in the competition and human players.

Typical approaches to Starcraft bots include case-based reasoning \fillin{more citations} and hierarchical skill management \fillin{citations}. The former technique aims to learn skills, build order, and general strategy from replays of past games, while the latter technique organizes expert modules into a hierarchical decision model. These two approaches need not be mutually exclusive. Wender and Watson refine the problem of creating expert modules by focusing exclusively on small-scale combat tactics in \cite{rl_small_scale_combat}, and it is this work that relates most closely to our own. While Wender and Watson restrict their analysis to the particular combat configuration of a single, fast, ranged unit versus multiple slower short-ranged units, we seek to optimize small-scale combat in a more general sense by allowing for heterogeneous unit groups with an unspecified number of combatants. In this way, our approach moves toward a more flexible solution for the problem of micro-management in small-scale combat. 

Another notable difference between our work and that of \cite{rl_small_scale_combat} is our use of NEAT (see Section \ref{sec:neat}) for learning unit controllers. We select NEAT for a number of reasons. First, as we will describe in Section \ref{sec:net_design}, our state space is continuous and high-dimensional. Neural networks are naturally well-designed for parsing continuous state spaces and determining their value. Second, due to the computational cost of communicating with and executing commands on the Starcraft runtime, we are restricted in the number of iterations we may perform to learn this state space. NEAT has been shown to significantly outperform traditional reinforcement learning methods under these circumstances \cite{Stanley:2004:EEN:1048234}. Finally, while we can accurately determine the value of a match, we cannot predict what intermediate behaviors might be optimal for our overall goal of achieving combat prowess. We therefore rely on NEAT to discover these innovations in the form of connections and connection weights in an efficient manner.

\section{Technical Approach}
\label{sec:tech}

We now cover our approach to the small-scale combat problem, beginning with a short introduction to NeuroEvolution of Augmenting Topologies (NEAT). We continue with a discussion of our implementation and network designs before proceeding to our software architecture.

\subsection{The NEAT Algorithm}
\label{sec:neat}

NEAT provides us with a fairly simple way of creating network designs without prior knowledge of the optimal target configuration. While traditional neural network algorithms, such as backpropagation, rely on a predefined network architecture, NEAT requires only that we specify a relatively small number of parameters, shown in Table \ref{tab:neat_params}.

\begin{table}
\centering
\begin{tabular}{|l|l|}
	\hline
	{\bf Parameter} & {\bf Interpretation}\\ \hline
	Input Nodes & Game State Representation\\ \hline
	Output Nodes & Available Actions and Decisions\\ \hline
	\multirow{2}{*}{Evolution Parameters} &  Intensity and Character of\\
	& Behavior Adjustments\\ \hline
\end{tabular}
\caption{Parameters specified for the NEAT algorithm.}
\label{tab:neat_params}
\end{table}

\subsection{Network Design}
\label{sec:net_design}

\section{Software}
\label{sec:soft}

\section{Experiments}
\label{sec:exp}

\section{Conclusions}
\label{sec:conclusion}

\section{Future Work}
\label{sec:future}

\bibliographystyle{unsrt}
\bibliography{bibliography}

\end{document}
